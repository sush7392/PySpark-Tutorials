{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "449e632a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb85835d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d4100a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Dataframe').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9a35ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.205:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Dataframe</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd25811d160>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c46cb2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+\n",
      "|   Name|Age |Experience|\n",
      "+-------+----+----------+\n",
      "|Sushant|  20|        10|\n",
      "| Roshan|  17|        20|\n",
      "|Shamita|  17|         5|\n",
      "| Aakash|  19|         8|\n",
      "+-------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark = spark.read.option('header','true').csv('/home/rosh/Documents/testing.csv', inferSchema=True)\n",
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3793776d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age : integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking the schemas\n",
    "\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab6bc7e",
   "metadata": {},
   "source": [
    "## Note :- By default the pyspark takes all the values as the string. So if we do not want that                  pyspark should take all the values as default i.e as string we have to give one                          more function in csv and that is inferSchema and set the values as TRUE for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2500f489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+\n",
      "|   Name|Age |Experience|\n",
      "+-------+----+----------+\n",
      "|Sushant|  20|        10|\n",
      "| Roshan|  17|        20|\n",
      "|Shamita|  17|         5|\n",
      "| Aakash|  19|         8|\n",
      "+-------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SOME ANOTHER WAYS OF GETTING THE DATA IN PYSPARKM\n",
    "\n",
    "df_pyspark = spark.read.csv('/home/rosh/Documents/testing.csv',header=True,inferSchema=True)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac5de33",
   "metadata": {},
   "source": [
    "## Note :- Dataframe is a datastructures, because inside this you can perform various operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "189bac88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'Age ', 'Experience']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the column name\n",
    "\n",
    "df_pyspark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6b98e84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Sushant', Age =20, Experience=10),\n",
       " Row(Name='Roshan', Age =17, Experience=20),\n",
       " Row(Name='Shamita', Age =17, Experience=5),\n",
       " Row(Name='Aakash', Age =19, Experience=8)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the head element. So basically in pyspark we get the head elemnt in the type of list\n",
    "\n",
    "df_pyspark.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5feff5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   Name|\n",
      "+-------+\n",
      "|Sushant|\n",
      "| Roshan|\n",
      "|Shamita|\n",
      "| Aakash|\n",
      "+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the single column i.e getting the value of one column. so for that we use the select function.And we \n",
    "# are also getting its type\n",
    "\n",
    "ty_pe = df_pyspark.select('Name')\n",
    "ty_pe.show()\n",
    "type(ty_pe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9e38e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|   Name|Experience|\n",
      "+-------+----------+\n",
      "|Sushant|        10|\n",
      "| Roshan|        20|\n",
      "|Shamita|         5|\n",
      "| Aakash|         8|\n",
      "+-------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting multiple columns from the data\n",
    "\n",
    "ty_pe = df_pyspark.select('Name','Experience')\n",
    "ty_pe.show()\n",
    "type(ty_pe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f30ccb5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'), ('Age ', 'int'), ('Experience', 'int')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the data types. so for that we use tge dtypes functon\n",
    "\n",
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "160c9466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+-----------------+\n",
      "|summary|   Name| Age |       Experience|\n",
      "+-------+-------+-----+-----------------+\n",
      "|  count|      4|    4|                4|\n",
      "|   mean|   null|18.25|            10.75|\n",
      "| stddev|   null|  1.5|6.499999999999999|\n",
      "|    min| Aakash|   17|                5|\n",
      "|    max|Sushant|   20|               20|\n",
      "+-------+-------+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describing our data\n",
    "\n",
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f42f4339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding columns in data frame in pyspark. so for that we use the withColumn function\n",
    "\n",
    "Add_column = df_pyspark.withColumn('Experience After 2 Year',df_pyspark['Experience']+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "191478bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+-----------------------+\n",
      "|   Name|Age |Experience|Experience After 2 Year|\n",
      "+-------+----+----------+-----------------------+\n",
      "|Sushant|  20|        10|                     12|\n",
      "| Roshan|  17|        20|                     22|\n",
      "|Shamita|  17|         5|                      7|\n",
      "| Aakash|  19|         8|                     10|\n",
      "+-------+----+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Add_column.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba21814f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+\n",
      "|   Name|Age |Experience|\n",
      "+-------+----+----------+\n",
      "|Sushant|  20|        10|\n",
      "| Roshan|  17|        20|\n",
      "|Shamita|  17|         5|\n",
      "| Aakash|  19|         8|\n",
      "+-------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dropping the column in pyspark os for that we use drop function\n",
    "\n",
    "drop_column = df_pyspark.drop('Experience After 2 Year')\n",
    "drop_column.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9b96368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+\n",
      "|New Name|Age |Experience|\n",
      "+--------+----+----------+\n",
      "| Sushant|  20|        10|\n",
      "|  Roshan|  17|        20|\n",
      "| Shamita|  17|         5|\n",
      "|  Aakash|  19|         8|\n",
      "+--------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Renaming the column name so for that we use the withColumnRenamed function.\n",
    "\n",
    "rename_column = df_pyspark.withColumnRenamed('Name','New Name')\n",
    "rename_column.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c014c2a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
